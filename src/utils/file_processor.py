# src/utils/file_processor.py
"""é‡æ„åçš„æ–‡ä»¶å¤„ç†å™¨"""
import json
import shutil
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
from datetime import datetime

from src.config import config_manager
from src.utils.data_downloader import DownloadService


class FileProcessor:
    """æ–‡ä»¶å¤„ç†å™¨ - å•ä¸€èŒè´£ï¼šå¤„ç†æ–‡ä»¶æ“ä½œ"""

    def __init__(self):
        self.file_config = config_manager.file

    def clean_character_files(self) -> Dict[str, Any]:
        """æ¸…ç†è§’è‰²æ–‡ä»¶ä¸­çš„å†—ä½™å­—æ®µ"""
        character_files = self.file_config.list_character_files()

        if not character_files:
            return {"processed": 0, "errors": [], "backup_created": False}

        print(f"ğŸ§¹ å¼€å§‹æ¸…ç† {len(character_files)} ä¸ªè§’è‰²æ–‡ä»¶...")

        # åˆ›å»ºå¤‡ä»½
        backup_path = self._create_backup(character_files)

        processed_count = 0
        error_files = []

        for file_path in character_files:
            try:
                success = self._clean_single_file(file_path)
                if success:
                    processed_count += 1
                else:
                    error_files.append(str(file_path))
            except Exception as e:
                print(f"âŒ æ¸…ç†å¤±è´¥ {file_path}: {e}")
                error_files.append(str(file_path))

        self._print_processing_summary(processed_count, error_files, backup_path)

        return {
            "processed": processed_count,
            "errors": error_files,
            "backup_created": backup_path is not None,
            "backup_path": str(backup_path) if backup_path else None
        }

    def validate_character_files(self) -> Dict[str, Any]:
        """éªŒè¯è§’è‰²æ–‡ä»¶æœ‰æ•ˆæ€§"""
        character_files = self.file_config.list_character_files()

        if not character_files:
            return {"valid": 0, "invalid": 0, "errors": []}

        print(f"ğŸ” éªŒè¯ {len(character_files)} ä¸ªè§’è‰²æ–‡ä»¶...")

        valid_files = []
        invalid_files = []
        error_details = []

        for file_path in character_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # æ£€æŸ¥å¿…éœ€å­—æ®µ
                required_fields = ["Id", "Name", "Stats"]
                if all(field in data for field in required_fields):
                    valid_files.append(str(file_path))
                else:
                    invalid_files.append(str(file_path))
                    missing = [field for field in required_fields if field not in data]
                    error_details.append(f"{file_path.name}: ç¼ºå°‘å­—æ®µ {missing}")

            except Exception as e:
                invalid_files.append(str(file_path))
                error_details.append(f"{file_path.name}: è§£æé”™è¯¯ {e}")

        print(f"ğŸ“Š éªŒè¯å®Œæˆ: æœ‰æ•ˆ {len(valid_files)} ä¸ª, æ— æ•ˆ {len(invalid_files)} ä¸ª")

        return {
            "valid": len(valid_files),
            "invalid": len(invalid_files),
            "valid_files": valid_files,
            "invalid_files": invalid_files,
            "error_details": error_details
        }

    def create_backup(self, backup_name: str = None) -> str:
        """åˆ›å»ºå¤‡ä»½"""
        if backup_name is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"characters_backup_{timestamp}"

        backup_path = self.file_config.get_backup_path(backup_name)
        character_files = self.file_config.list_character_files()

        backup_path.mkdir(parents=True, exist_ok=True)

        for file_path in character_files:
            backup_file = backup_path / file_path.name
            shutil.copy2(file_path, backup_file)

        print(f"ğŸ’¾ å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_path}")
        return str(backup_path)

    def restore_backup(self, backup_name: str) -> bool:
        """ä»å¤‡ä»½æ¢å¤"""
        backup_path = self.file_config.get_backup_path(backup_name)

        if not backup_path.exists():
            print(f"âŒ å¤‡ä»½ä¸å­˜åœ¨: {backup_path}")
            return False

        print(f"ğŸ”„ ä»å¤‡ä»½æ¢å¤: {backup_path}")

        try:
            # æ¸…ç©ºå½“å‰è§’è‰²ç›®å½•
            for file_path in self.file_config.list_character_files():
                file_path.unlink()

            # ä»å¤‡ä»½å¤åˆ¶æ–‡ä»¶
            for backup_file in backup_path.glob("*.json"):
                target_path = self.file_config.characters_dir / backup_file.name
                shutil.copy2(backup_file, target_path)

            print(f"âœ… æ¢å¤å®Œæˆ: ä» {backup_path} æ¢å¤äº†æ‰€æœ‰æ–‡ä»¶")
            return True

        except Exception as e:
            print(f"âŒ æ¢å¤å¤±è´¥: {e}")
            return False

    def list_backups(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½"""
        backups = []

        if not self.file_config.backup_dir.exists():
            return backups

        for backup_path in self.file_config.backup_dir.iterdir():
            if backup_path.is_dir():
                backup_files = list(backup_path.glob("*.json"))
                created_time = datetime.fromtimestamp(backup_path.stat().st_ctime)

                backups.append({
                    "name": backup_path.name,
                    "path": str(backup_path),
                    "file_count": len(backup_files),
                    "created_time": created_time,
                    "size_mb": sum(f.stat().st_size for f in backup_files) / 1024 / 1024
                })

        # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
        backups.sort(key=lambda x: x["created_time"], reverse=True)
        return backups

    def get_file_statistics(self) -> Dict[str, Any]:
        """è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯"""
        character_files = self.file_config.list_character_files()

        total_size = sum(f.stat().st_size for f in character_files)
        file_sizes = [f.stat().st_size for f in character_files]

        return {
            "total_files": len(character_files),
            "total_size_mb": total_size / 1024 / 1024,
            "average_size_kb": (sum(file_sizes) / len(file_sizes)) / 1024 if file_sizes else 0,
            "largest_file_kb": max(file_sizes) / 1024 if file_sizes else 0,
            "smallest_file_kb": min(file_sizes) / 1024 if file_sizes else 0
        }

    def clean_icon_fields(self, file_path: Path) -> bool:
        """æ¸…ç†å•ä¸ªæ–‡ä»¶çš„Iconå­—æ®µ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æ¸…ç†Iconå­—æ®µ
            cleaned_data = self._remove_redundant_fields(data)

            # å†™å›æ–‡ä»¶
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(cleaned_data, f, ensure_ascii=False, indent=2)

            print(f"âœ… æ¸…ç†æˆåŠŸ: {file_path}")
            return True

        except Exception as e:
            print(f"âŒ æ¸…ç†å¤±è´¥ {file_path}: {e}")
            return False

    def _clean_single_file(self, file_path: Path) -> bool:
        """æ¸…ç†å•ä¸ªæ–‡ä»¶"""
        return self.clean_icon_fields(file_path)

    def _remove_redundant_fields(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ç§»é™¤å†—ä½™å­—æ®µ - ä½¿ç”¨ss.pyçš„æœ‰æ•ˆé€»è¾‘"""

        # ä½¿ç”¨ss.pyçš„æ’é™¤å­—æ®µåˆ—è¡¨
        exclude_fields = {"Icon", "PartnerInfo", "Skin", "LevelEXP", "Skill",
                          "SkillList", "Talent", "Potential", "PotentialDetail",
                          "Image", "Thumbnail"}
        exclude_parts = {"Part4", "Part5", "Part6", "PartSub"}

        def clean_recursive(obj):
            if isinstance(obj, dict):
                cleaned = {}
                for key, value in obj.items():
                    # æ’é™¤æŒ‡å®šå­—æ®µ
                    if key in exclude_fields:
                        continue

                    # ç‰¹æ®Šå¤„ç†SpecialElementType
                    elif key == "SpecialElementType":
                        if isinstance(value, dict):
                            cleaned[key] = {}
                            for sub_key, sub_value in value.items():
                                if sub_key != "Icon":
                                    cleaned[key][sub_key] = sub_value
                        else:
                            cleaned[key] = value

                    # ç‰¹æ®Šå¤„ç†FairyRecommend
                    elif key == "FairyRecommend":
                        if isinstance(value, dict):
                            cleaned[key] = {}
                            for part_key, part_value in value.items():
                                if part_key not in exclude_parts:
                                    cleaned[key][part_key] = clean_recursive(part_value)
                                else:
                                    if isinstance(part_value, dict):
                                        cleaned[key][part_key] = {}
                                        for attr_key, attr_value in part_value.items():
                                            if attr_key != "Icon":
                                                cleaned[key][part_key][attr_key] = attr_value
                                    else:
                                        cleaned[key][part_key] = part_value
                        else:
                            cleaned[key] = value

                    else:
                        cleaned[key] = clean_recursive(value)
                return cleaned
            elif isinstance(obj, list):
                return [clean_recursive(item) for item in obj]
            else:
                return obj

        return clean_recursive(data)

    def _create_backup(self, files: List[Path]) -> Path:
        """åˆ›å»ºå¤‡ä»½"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"pre_clean_backup_{timestamp}"
        return Path(self.create_backup(backup_name))

    def _print_processing_summary(self, processed_count: int, error_files: List[str], backup_path: Optional[Path]):
        """æ‰“å°å¤„ç†æ€»ç»“"""
        print("\n" + "=" * 60)
        print("ğŸ“Š å¤„ç†å®Œæˆ!")
        print(f"âœ… æˆåŠŸå¤„ç†: {processed_count} ä¸ªæ–‡ä»¶")
        print(f"âŒ å¤„ç†å¤±è´¥: {len(error_files)} ä¸ªæ–‡ä»¶")

        if backup_path:
            print(f"ğŸ’¾ å¤‡ä»½ä½ç½®: {backup_path}")

        if error_files:
            print("\nå¤±è´¥çš„æ–‡ä»¶:")
            for error_file in error_files:
                print(f"  - {error_file}")


class FileManagementService:
    """æ–‡ä»¶ç®¡ç†æœåŠ¡ - æä¾›å®Œæ•´çš„æ•°æ®ç®¡ç†åŠŸèƒ½"""

    def __init__(self):
        self.processor = FileProcessor()
        self.download_service = DownloadService()

    def initialize_data_directory(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–æ•°æ®ç›®å½• - è¿”å›æ›´è¯¦ç»†çš„ç»“æœ"""
        result = {
            "success": True,
            "directories_created": [],
            "directories_existing": [],
            "data_completeness": {},
            "warnings": []
        }

        print("ğŸ“ åˆå§‹åŒ–æ•°æ®ç›®å½•ç»“æ„...")

        # æ£€æŸ¥ç›®å½•ç»“æ„
        structure_check = self.processor.file_config.validate_data_structure()

        if not structure_check["valid"]:
            print("âš ï¸ æ•°æ®ç›®å½•ç»“æ„ä¸å®Œæ•´ï¼Œæ­£åœ¨ä¿®å¤...")

            # åˆ›å»ºç¼ºå¤±çš„ç›®å½•
            for dir_name in structure_check["missing_dirs"]:
                dir_path = getattr(self.processor.file_config, f"{dir_name}")
                dir_path.mkdir(parents=True, exist_ok=True)
                result["directories_created"].append(str(dir_path))
                print(f"âœ… åˆ›å»ºç›®å½•: {dir_path}")
        else:
            print("âœ… æ•°æ®ç›®å½•ç»“æ„å®Œæ•´")
            # è®°å½•å·²å­˜åœ¨çš„ç›®å½•
            for dir_name, dir_info in structure_check["details"].items():
                if dir_info["exists"]:
                    result["directories_existing"].append(dir_info["path"])

        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§ ä¸equipmentçš„é€‚é…æœ‰é—®é¢˜ï¼Œåé¢ä¿®æ”¹
        # completeness = self.download_service.check_data_completeness()
        # result["data_completeness"] = completeness
        #
        # if completeness["completion_rate"] < 100:
        #     warning_msg = f"æ•°æ®ä¸å®Œæ•´: {completeness['completion_rate']:.1f}%"
        #     result["warnings"].append(warning_msg)
        #     print(f"âš ï¸ {warning_msg}")

        # æ–‡ä»¶ç»Ÿè®¡
        stats = self.processor.get_file_statistics()
        result["file_statistics"] = stats

        print(f"ğŸ“Š æ–‡ä»¶ç»Ÿè®¡: {stats['total_files']} ä¸ªè§’è‰²æ–‡ä»¶, æ€»å¤§å°: {stats['total_size_mb']:.1f}MB")

        return result

    def download_all_data(self) -> Dict[str, Any]:
        """ä¸‹è½½æ‰€æœ‰æ•°æ®ï¼ˆå®Œæ•´æµç¨‹ï¼‰"""
        print("ğŸ“¥ å¼€å§‹ä¸‹è½½æ‰€æœ‰æ•°æ®...")

        result = self.download_service.download_all_data()

        if result["character_data_success"]:
            print(f"âœ… ä¸‹è½½å®Œæˆ: {result['downloaded_count']}/{result['total_characters']} ä¸ªè§’è‰²")
        else:
            print("âŒ ä¸‹è½½å¤±è´¥")

        return result

    def download_character_list(self) -> bool:
        """ä¸‹è½½è§’è‰²åˆ—è¡¨"""
        print("ğŸ“‹ ä¸‹è½½è§’è‰²åˆ—è¡¨...")

        success = self.download_service.downloader.download_character_list()

        if success:
            print("âœ… è§’è‰²åˆ—è¡¨ä¸‹è½½æˆåŠŸ")
        else:
            print("âŒ è§’è‰²åˆ—è¡¨ä¸‹è½½å¤±è´¥")

        return success is not None

    def download_missing_characters(self) -> Tuple[int, List[str]]:
        """ä¸‹è½½ç¼ºå¤±çš„è§’è‰²æ•°æ®"""
        print("ğŸ” æ£€æŸ¥ç¼ºå¤±çš„è§’è‰²æ•°æ®...")

        completeness = self.download_service.check_data_completeness()

        if completeness["completion_rate"] == 100:
            print("âœ… æ²¡æœ‰ç¼ºå¤±çš„è§’è‰²æ•°æ®")
            return 0, []

        print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½ {len(completeness['missing_files'])} ä¸ªç¼ºå¤±çš„è§’è‰²...")

        success_count, failed_ids = self.download_service.downloader.batch_download_characters(
            completeness["missing_files"]
        )

        print(f"ğŸ“Š ä¸‹è½½å®Œæˆ: æˆåŠŸ {success_count} ä¸ª, å¤±è´¥ {len(failed_ids)} ä¸ª")

        return success_count, failed_ids

    def retry_failed_downloads(self, max_retries: int = 3) -> Tuple[int, List[str]]:
        """é‡è¯•å¤±è´¥çš„ä¸‹è½½"""
        print("ğŸ”„ é‡è¯•å¤±è´¥çš„ä¸‹è½½...")

        success_count, still_failed = self.download_service.downloader.retry_failed_downloads(max_retries)

        print(f"ğŸ“Š é‡è¯•å®Œæˆ: æˆåŠŸ {success_count} ä¸ª, ä»ç„¶å¤±è´¥ {len(still_failed)} ä¸ª")

        return success_count, still_failed

    def test_network_connection(self) -> bool:
        """æµ‹è¯•ç½‘ç»œè¿æ¥"""
        print("ğŸ”— æµ‹è¯•ç½‘ç»œè¿æ¥...")

        success = self.download_service.downloader.test_connection()

        if success:
            print("âœ… ç½‘ç»œè¿æ¥æ­£å¸¸")
        else:
            print("âŒ ç½‘ç»œè¿æ¥å¤±è´¥")

        return success

    def perform_maintenance(self) -> Dict[str, Any]:
        """æ‰§è¡Œç»´æŠ¤ä»»åŠ¡"""
        result = {}

        print("ğŸ”§ æ‰§è¡Œç³»ç»Ÿç»´æŠ¤...")

        # éªŒè¯æ–‡ä»¶
        validation_result = self.processor.validate_character_files()
        result["validation"] = validation_result

        print(f"ğŸ“‹ æ–‡ä»¶éªŒè¯: {validation_result['valid']} ä¸ªæœ‰æ•ˆ, {validation_result['invalid']} ä¸ªæ— æ•ˆ")

        # æ¸…ç†æ–‡ä»¶
        clean_result = self.processor.clean_character_files()
        result["cleaning"] = clean_result

        # æ–‡ä»¶ç»Ÿè®¡
        stats = self.processor.get_file_statistics()
        result["statistics"] = stats

        print("âœ… ç³»ç»Ÿç»´æŠ¤å®Œæˆ")

        return result

    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        status = {
            "file_system": {},
            "data_status": {},
            "network_status": "unknown",
            "recommendations": []
        }

        # æ–‡ä»¶ç³»ç»ŸçŠ¶æ€
        structure = self.processor.file_config.validate_data_structure()
        status["file_system"] = {
            "valid": structure["valid"],
            "details": structure["details"]
        }

        # æ•°æ®çŠ¶æ€
        completeness = self.download_service.check_data_completeness()
        status["data_status"] = completeness

        # ç½‘ç»œçŠ¶æ€
        try:
            network_ok = self.test_network_connection()
            status["network_status"] = "connected" if network_ok else "disconnected"
        except:
            status["network_status"] = "unknown"

        # ç”Ÿæˆå»ºè®®
        if not structure["valid"]:
            status["recommendations"].append("ä¿®å¤æ•°æ®ç›®å½•ç»“æ„")

        if completeness["completion_rate"] < 100:
            if status["network_status"] == "connected":
                status["recommendations"].append("ä¸‹è½½ç¼ºå¤±çš„è§’è‰²æ•°æ®")
            else:
                status["recommendations"].append("æ£€æŸ¥ç½‘ç»œè¿æ¥åä¸‹è½½ç¼ºå¤±æ•°æ®")

        if completeness["completion_rate"] == 0:
            status["recommendations"].append("è¿è¡Œå®Œæ•´çš„æ•°æ®ä¸‹è½½æµç¨‹")

        # æ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥çš„ä¸‹è½½éœ€è¦é‡è¯•
        failed_downloads_file = self.processor.file_config.failed_downloads_file
        if failed_downloads_file.exists():
            try:
                import json
                with open(failed_downloads_file, 'r', encoding='utf-8') as f:
                    failed_ids = json.load(f)
                if failed_ids:
                    status["recommendations"].append(f"é‡è¯• {len(failed_ids)} ä¸ªå¤±è´¥çš„ä¸‹è½½")
            except:
                pass

        return status

    def cleanup_system(self) -> Dict[str, Any]:
        """æ¸…ç†ç³»ç»Ÿ"""
        result = {
            "cache_cleared": False,
            "backups_cleaned": 0,
            "temp_files_removed": 0
        }

        print("ğŸ§¹ ç³»ç»Ÿæ¸…ç†...")

        # æ¸…ç©ºç¼“å­˜ç›®å½•
        cache_dir = self.processor.file_config.cache_dir
        if cache_dir.exists():
            import shutil
            shutil.rmtree(cache_dir)
            cache_dir.mkdir()
            result["cache_cleared"] = True
            print("âœ… ç¼“å­˜å·²æ¸…ç©º")

        # æ¸…ç†æ—§çš„å¤‡ä»½ï¼ˆä¿ç•™æœ€è¿‘5ä¸ªï¼‰
        backups = self.processor.list_backups()
        if len(backups) > 5:
            backups_to_remove = backups[5:]
            for backup in backups_to_remove:
                import shutil
                shutil.rmtree(backup["path"])
                result["backups_cleaned"] += 1
            print(f"âœ… æ¸…ç†äº† {result['backups_cleaned']} ä¸ªæ—§å¤‡ä»½")

        # æ¸…ç©ºè®¡ç®—å™¨ç¼“å­˜
        from src.core.service_factory import get_service_factory
        service_factory = get_service_factory()
        service_factory.clear_cache()
        print("âœ… è®¡ç®—å™¨ç¼“å­˜å·²æ¸…ç©º")

        print("âœ… ç³»ç»Ÿæ¸…ç†å®Œæˆ")
        return result

    def export_data(self, export_path: str = None) -> str:
        """å¯¼å‡ºæ•°æ®"""
        import shutil
        from datetime import datetime

        if export_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            export_path = f"zzz_calculator_export_{timestamp}"

        export_dir = Path(export_path)
        export_dir.mkdir(parents=True, exist_ok=True)

        # å¤åˆ¶è§’è‰²æ•°æ®
        characters_dir = export_dir / "characters"
        shutil.copytree(self.processor.file_config.characters_dir, characters_dir)

        # å¤åˆ¶é…ç½®æ–‡ä»¶
        config_files = [
            self.processor.file_config.character_ids_file,
            self.processor.file_config.id_name_mapping_file
        ]

        for config_file in config_files:
            if config_file.exists():
                shutil.copy2(config_file, export_dir / config_file.name)

        print(f"âœ… æ•°æ®å·²å¯¼å‡ºåˆ°: {export_path}")
        return str(export_dir)